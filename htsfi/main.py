import os
from shutil import copyfile, copy2
import numpy as np
from scipy.linalg import eig, eigh, cholesky, svd, eigvals, schur

import pandas as pd
import requests

import matplotlib.pyplot as plt
from matplotlib.dates import YearLocator, DateFormatter
from matplotlib.ticker import FuncFormatter

PDs = np.around(
    np.array([
    0.01, 0.03, 0.04, 0.04, 0.05, 0.08, 0.10, 0.13, 0.17, 0.24, 0.34, 
    0.53, 0.73, 1.40, 2.45, 6.00, 9.57, 16.72, 33.32
    ]) / 100,
5)
ODRs = [10, 21, 24, 27, 31, 34, 37, 41, 44, 47, 51, 54, 57, 61, 64, 67, 70, 75, 80]
SPs = np.array([
    'AAA', 'AA+', 'AA', 'AA-', 'A+', 'A', 'A-', 'BBB+', 'BBB', 'BBB-', 'BB+', 'BB',
    'BB-', 'B+', 'B', 'B-', 'CCC+', 'CCC-', 'CC'
])
SPRAT_KEY = {rat: p_of_d for rat, p_of_d in zip(SPs, PDs)}
PD_KEY = {v:k for k,v in SPRAT_KEY.items()}

def move_to_doc_folder(filename):
    dir_ = os.getcwd()
    dir_docs = dir_ + '/docs/'
    print (dir_ + '/' + filename, dir_docs + filename)
    copy2(dir_ + '/' + filename, dir_docs + filename)

import os
import glob
import shutil
from pathlib import Path

import matplotlib

def update_style():
    mpldir = os.path.join(matplotlib.get_configdir(), 'stylelib')

    mplpath = Path(mpldir)
    if not mplpath.exists():
        mplpath.mkdir()

    PATH = os.getcwd()
    EXT = 'mplstyle'
    style_files = glob.glob(os.path.join(PATH, f'*.{EXT}'))

    for _path_file in style_files:
        _, fname = os.path.split(_path_file)
        dest = os.path.join(mpldir, fname)
        shutil.copy(_path_file, dest)
        
    matplotlib.pyplot.style.reload_library()

def make_warning(title, body, additional = ''):
    html = '<div class="admonition warning">'
    html += f'<p class="admonition-title fa fa-exclamation-circle">{title}</p>'
    html += f'<p>{body}</p>'
    html += additional
    html += '</div>'
    
    return html

def alpha_from_descript(mean, var):
    val = mean*(1-mean) / var
    return mean*(val - 1)

def beta_from_descript(mean, var):
    val = mean*(1-mean) / var
    return (1 - mean)*(val - 1)    

def beta_params_from_descript(mean, var):
    return alpha_from_descript(mean, var), beta_from_descript(mean, var)

def loan_ul(EAD, p_of_d, lgd, p_of_d_var, lgd_var):
    return EAD*np.sqrt(p_of_d*lgd_var + lgd*p_of_d_var)

def corrs_to_corrmat(p):
    """
    Returns nxn matrix of correlation coffecients
    
    Params:
        p,    nx1 array of correlation coeffecients
    """
    assert len(p.shape) == 1 or p.shape[1] == 1, '`p` must be an nx1 array'
    
    n = p.size
    eye = np.eye(n)
    inveye = np.logical_not(eye).astype(np.int8)
    idx = np.indices((n,n)).sum(axis=0) - 1
    idx_flat = np.where(idx.flatten() >= n, -1, idx.flatten())
    corrmat = p[idx_flat].reshape(n,n)*inveye + eye
    
    return corrmat

def port_ul_w_corr(w, ul, p):
    """
    Find unexpected loss of portfolio when unexpected losses are
    correlated.
    
    Params
        w:    array; weights of each loan in whole portfolio
        ul:   array; unexpected loss of each loan
        p:    array; correlation coefficients of unexpected loss of each loan
        
    Process
        1) convert correlation coefficients into nxn matrix
        2) complete the multiplications first, then sum the matrix thereafter
            a) multiply each weight by every other weight (outer multiplication)
            b) multiply each unexpected loss by every other
            c) multiply weight matrix by the unexpected loss matrix by correlation matrix
        3) this creates nxn matrix of correlated unexpected loss terms that would
           be generated by the sum iteration. so SUM!
        4) take square root
    """
    corrmat = corrs_to_corrmat(p)
    w_outer = np.outer(w,w)
    ul_outer = np.outer(ul, ul)
    lossmat = corrmat @ w_outer @ ul_outer
    
    return np.sqrt(lossmat.sum())    

def fix_corrmat(A):
    LAM, S = schur(A)
    LAM = np.diag(np.diagonal(LAM))
    assert np.all(np.isclose(A, S @ LAM @ np.linalg.inv(S)))

    LAM_p = np.where(LAM < 0, 0, LAM)

    scalmat = np.zeros_like(np.diag(LAM_p))
    for i in range(S.shape[0]):
        scalmat[i] = 1/np.sum(S[i]**2*np.diag(LAM_p))
    T = np.diag(scalmat)

    B = np.sqrt(T) @ S @ np.sqrt(LAM_p)

    A_p = B @ B.T
    
    return A_p

def get_yields():
    series = [
        'DGS10',
        'BAMLC0A1CAAAEY',
        'BAMLC0A2CAAEY',
        'BAMLC0A3CAEY',
        'BAMLC0A4CBBBEY',
        'BAMLH0A1HYBBEY',
        'BAMLH0A2HYBEY',
        'BAMLH0A3HYCEY',
    ]

    frames = [framer(ser) for ser in series]

    yields = frames[0]
    for f in frames[1:]:
        yields = yields.merge(f, on='Date')
    yields = yields.set_index('Date')

    yields = yields.apply(numeric_coerce)
    yields.columns = ['US10', 'AAA', 'AA', 'A', 'BBB', 'BB', 'B', 'C']
    yields /= 100
    
    return yields

def serurl(ser):
    fredkey = '9bf850a7158087dbc63ff13ae0e4dc4a'
    return f'https://api.stlouisfed.org/fred/series/observations?series_id={ser}&api_key={fredkey}&file_type=json'

def framer(ser):
    res = requests.get(serurl(ser))
    df = pd.DataFrame(res.json()['observations'])
    df = df.drop(['realtime_start', 'realtime_end'], axis=1)
    df.columns = ['Date', ser]
    df.Date = pd.to_datetime(df.Date)

    return df

def numeric_coerce(s):
    return pd.to_numeric(s, errors='coerce')
